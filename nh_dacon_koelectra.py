# -*- coding: utf-8 -*-
"""NH_dacon_KoELECTRA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1agtXlM3DGo_308qmZFv4roWqDrkRCrG0
"""

from google.colab import drive
drive.mount("/content/drive")

import os
from pathlib import Path

current_path = Path(os.getcwd())
print(current_path)

base_path = current_path / "drive" / "My Drive" /"NH_dacon"
os.chdir(base_path)

!pip install transformers

!pip install ray

!pip install tensorboardX

from functools import partial
import pandas as pd
import numpy as np
from collections import Counter

import re 
import time
# import nltk
import random
import datetime
from tqdm import tqdm

import torch

from transformers import ElectraModel, ElectraForSequenceClassification, ElectraTokenizer, get_linear_schedule_with_warmup, AdamW
from torch.utils.data import TensorDataset, DataLoader,RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# vocab tokenization
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import re
import unicodedata
import six
# from konlpy.tag import Mecab

from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler

!pwd

# test = pd.read_csv('./news_test.csv')
# submission = pd.read_csv('/content/sample_submission.csv')

# train.head(1)

# print(train.shape)
# print(test.shape)

# sentences[:10]

# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=False)
# tokenizer = FullTokenizer(vocab_file='/content/vocab.txt', do_lower_case=False)

# print(sentences[0])
# # print(tokenizer.tokenize('코스닥 기관 현재 678억 순매도'))
# print(tokenized_texts[0])

# sentences = ["[CLS]"+str(sentence)+"[SEP]" for sentence in sentences]
# sentences[:5]
# for tokens in tokenized_texts:
#   tokens.insert(0, "[CLS]")
#   tokens.append("[SEP]")
# sentences = ["[CLS]"+str(sentence)+ for sentence in sentences]
# tokenized_texts[:5]

# print(attention_masks[0])

# print(train_inputs[0])
# print(train_labels[0])
# print(train_masks[0])
# print(validation_inputs[0])
# print(validation_labels[0])
# print(validation_masks[0])

"""#training"""

# t_sentences = [normalize_text(sentence) for sentence in t_sentences]
# t_sentences[:5]

def load_data(data_dir="./data"):
  train_data = pd.read_csv(data_dir+"/news_train.csv")
  return train_data

def token_to_ids(sentences, max_seq_len):
  tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")
  sentences = ["[CLS]"+str(sentence)+"[SEP]" for sentence in sentences]
  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
  # print('Max sentence length: ', sum([len(sen) for sen in input_ids])/len(input_ids))
  MAX_LEN = max_seq_len
  input_ids = pad_sequences(input_ids,maxlen=MAX_LEN, dtype="long",truncating = "post",padding = "post")
  # input_ids[0]
  return input_ids

def convert_to_tensordata(inputs, labels, masks):
  t_inputs = torch.tensor(inputs)
  t_labels = torch.tensor(labels)
  t_masks = torch.tensor(masks)

  return TensorDataset(t_inputs, t_masks, t_labels)

def flat_accuracy(preds, labels):
    
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    return np.sum(pred_flat == labels_flat) / len(labels_flat)

# 시간 표시 함수
def format_time(elapsed):

    # 반올림
    elapsed_rounded = int(round((elapsed)))
    
    # hh:mm:ss으로 형태 변경
    return str(datetime.timedelta(seconds=elapsed_rounded))

# 재현을 위해 랜덤시드 고정
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

def trainer(config, checkpoint_dir=None, data_dir=None):

  if torch.cuda.is_available():
    device = torch.device("cuda")
    print('the are %d GPU(s) abailable.'%torch.cuda.device_count())
    print('We will use the GPU:',torch.cuda.get_device_name(0))
  else:
    device = torch.device("cpu")
    print('No GPU available, using the CPU instead.')

  model = ElectraForSequenceClassification.from_pretrained("monologg/koelectra-small-v3-discriminator",num_labels=2)
  model.cuda()

  optimizer = AdamW(model.parameters(),
                    lr = config["lr"],
                    eps = 1e-8)

  if checkpoint_dir:
    model_state, optimizer_state = torch.load(
        os.path.join(checkpoint_dir, "checkpoint"))
    model.load_state_dict(model_state)
    optimizer.load_state_dict(optimizer_state)

  train = load_data(data_dir)
  sentences = train['content']
  labels = train['info'].values

  input_ids = token_to_ids(sentences,max_seq_len=int(config["max_seq_len"]))

  attention_masks=[]
  for seq in input_ids:
    seq_mask = [float(i>0) for i in seq]
    attention_masks.append(seq_mask)

  train_inputs, validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=2020,test_size=0.1)
  train_masks, validation_masks,_,_=train_test_split(attention_masks,input_ids,random_state=2020,test_size=0.1)


  train_data = convert_to_tensordata(train_inputs, train_labels, train_masks)
  train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=int(config["batch_size"]))

  validation_data = convert_to_tensordata(validation_inputs, validation_labels, validation_masks)
  validation_dataloader = DataLoader(validation_data, sampler=SequentialSampler(validation_data), batch_size=int(config["batch_size"]))

  epochs = 5
  total_steps = len(train_dataloader) * epochs
  print('total steps : ', total_steps)
  scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)

  # 그래디언트 초기화
  model.zero_grad()

  # 에폭만큼 반복
  for epoch_i in range(epochs):
      
      # ========================================
      #               Training
      # ========================================
      
      print("")
      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
      print('Training...')

      t0 = time.time()
      total_loss = 0
      model.train()
          
      for step, batch in enumerate(train_dataloader):
          if step % 500 == 0 and not step == 0:
              elapsed = format_time(time.time() - t0)
              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

          batch = tuple(t.to(device) for t in batch)

          b_input_ids, b_input_mask, b_labels = batch

          outputs = model(b_input_ids, 
                          token_type_ids=None, 
                          attention_mask=b_input_mask, 
                          labels=b_labels)

          loss = outputs[0]
          total_loss += loss.item()
          loss.backward()

          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

          optimizer.step()
          scheduler.step()
          model.zero_grad()

      train_loss = total_loss / len(train_dataloader)         

      print("")
      print("  Average training loss: {0:.2f}".format(train_loss))
      print("  Training epoch took: {:}".format(format_time(time.time() - t0)))
          
      # ========================================
      #               Validation
      # ========================================

      print("")
      print("Running Validation...")

      t0 = time.time()

      model.eval()

      n_batch = 0
      eval_loss, eval_accuracy = 0, 0
      nb_eval_steps, nb_eval_examples = 0, 0

      for batch in validation_dataloader:
          batch = tuple(t.to(device) for t in batch)

          b_input_ids, b_input_mask, b_labels = batch

          with torch.no_grad():     
              outputs = model(b_input_ids, 
                              token_type_ids=None, 
                              attention_mask=b_input_mask)

          logit = outputs[0]
          logits = logit.detach().cpu().numpy()

          label_ids = b_labels.to('cpu').numpy()

          tmp_eval_accuracy = flat_accuracy(logits, label_ids)
          eval_accuracy += tmp_eval_accuracy
          nb_eval_steps += 1

      # print("  Average validation loss: {0:.2f}".format(val_loss))
      print("  Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
      print("  Validation took: {:}".format(format_time(time.time() - t0)))

      with tune.checkpoint_dir(epoch_i) as checkpoint_dir:
          path = os.path.join(checkpoint_dir, "checkpoint")
          # output_file = "./NH_KoELECTRA_small_raytune_{}.pt".format(epoch_i + 1)
          torch.save((model.state_dict(), optimizer.state_dict()), path)

      tune.report(loss=train_loss, accuracy=eval_accuracy/nb_eval_steps)


print("")
print("Training complete!")

def main():
  data_dir = os.path.abspath("./data")
    # train = load_data('./news_train.csv')

  config = {
          "lr": tune.loguniform(1e-6, 1e-4),
          "batch_size": tune.choice([16, 32, 48, 64]),
          "max_seq_len": tune.choice([128, 384, 512])
      }

  scheduler = ASHAScheduler(
        metric="loss",
        mode="min",
        max_t=5,
        grace_period=1,
        reduction_factor=2)

  reporter = CLIReporter(
        parameter_columns=["lr", "batch_size","max_seq_len"],
        metric_columns=["loss", "accuracy", "training_iteration"])


  result = tune.run(
      partial(trainer, data_dir=data_dir),
      resources_per_trial={"gpu": 1},
      config=config,
      num_samples=20,
      scheduler=scheduler,
      progress_reporter=reporter)

  best_trial = result.get_best_trial("loss", "min", "last")
  print("Best trial config: {}".format(best_trial.config))
  print("Best trial final validation loss: {}".format(
      best_trial.last_result["loss"]))
  print("Best trial final validation accuracy: {}".format(
      best_trial.last_result["accuracy"]))

  # device = "cpu"
  # if torch.cuda.is_available():
  #     device = "cuda:0"

  # model = ElectraForSequenceClassification.from_pretrained("monologg/koelectra-small-v3-discriminator",num_labels=2)
  # model.to(device)

  # best_checkpoint_dir = best_trial.checkpoint.value
  # model_state, optimizer_state = torch.load(os.path.join(
  #     best_checkpoint_dir, "checkpoint"))
  # best_trained_model.load_state_dict(model_state)

  # val_acc = val_accuracy(best_trained_model, device)
  # print("Best trial val set accuracy: {}".format(val_acc))


if __name__ == "__main__":
    # You can change the number of GPUs per trial here:
    main()

"""Test Inference"""

test = pd.read_csv('./news_test.csv')

t_id = test['id'].values.astype(str)
t_id

import time
start = time.time()

## Library 불러오기
import pandas as pd
import numpy as np
# from collections import Counter

# import re 
import time
# import nltk
# import random
import datetime
# from tqdm import tqdm

# import tensorflow as tf
import torch
# ElectraModel
from transformers import ElectraForSequenceClassification, ElectraTokenizer, get_linear_schedule_with_warmup, AdamW
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
# from sklearn.model_selection import train_test_split

# vocab tokenization
# from __future__ import absolute_import
# from __future__ import division
# from __future__ import print_function

# import collections
# import re
# import unicodedata
# import six

## pos_Tagger, Tokenizer, pretrained_embedding, Model 불러오기
# load tokenizer
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")

# load model
model = ElectraForSequenceClassification.from_pretrained("monologg/koelectra-small-v3-discriminator",num_labels=2)
model.load_state_dict(torch.load("./NH_KoELECTRA_small_b24_v3_3.pt"))
model.cuda()

t_sentences = test['content']
# t_sentences[:10]

## 형태소 분석 + 전처리
def convert_input_data(sentences):

    # BERT의 토크나이저로 문장을 토큰으로 분리
    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]
     
    # 입력 토큰의 최대 시퀀스 길이
    MAX_LEN = 256

    # 토큰을 숫자 인덱스로 변환
    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
    
    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움
    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

    # 어텐션 마스크 초기화
    attention_masks = []

    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정
    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상
    for seq in input_ids:
        seq_mask = [float(i>0) for i in seq]
        attention_masks.append(seq_mask)

    # 데이터를 파이토치의 텐서로 변환
    inputs = torch.tensor(input_ids)
    masks = torch.tensor(attention_masks)

    return inputs, masks

test_inputs, test_masks = convert_input_data(t_sentences)
test_data = TensorDataset(test_inputs, test_masks)
test_sampler = SequentialSampler(test_data)
batch_size=42
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

# 시간 표시 함수
def format_time(elapsed):

    # 반올림
    elapsed_rounded = int(round((elapsed)))
    
    # hh:mm:ss으로 형태 변경
    return str(datetime.timedelta(seconds=elapsed_rounded))

model.eval()
eval_loss, eval_accuracy = 0, 0
nb_eval_steps, nb_eval_examples = 0, 0

result_dict = {"info":[],"id":[]}
result_dict["id"].extend(t_id)

# 데이터로더에서 배치만큼 반복하여 가져옴
for step, batch in enumerate(test_dataloader):
    t0 = time.time()
    # 경과 정보 표시
    if step % 100 == 0 and not step == 0:
        elapsed = format_time(time.time() - t0)
        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))

    # 배치를 GPU에 넣음
    batch = tuple(t.to(device) for t in batch)
    
    # 배치에서 데이터 추출
    b_input_ids, b_input_mask = batch
    
    # 그래디언트 계산 안함
    with torch.no_grad():     
        # Forward 수행
        outputs = model(b_input_ids, 
                        token_type_ids=None, 
                        attention_mask=b_input_mask)
    
    # 로스 구함
    logit = outputs[0]

    # CPU로 데이터 이동
    logits = logit.detach().cpu().numpy()
    # id = b_id.to('cpu').numpy()
    predicted_label = np.argmax(logits, axis=1).flatten()
    result_dict["info"].extend(predicted_label)
    
    
#     # 출력 로짓과 라벨을 비교하여 정확도 계산
#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)
#     eval_accuracy += tmp_eval_accuracy
#     nb_eval_steps += 1

# print("")
# print("Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
print("Test took: {:}".format(format_time(time.time() - t0)))

print(time.time() - start)

import csv
info = result_dict["info"]
id = result_dict["id"]
with open('./submission_koelectra_small_len256bs42_v3_epoch3.csv','w') as f:
    # fieldnames = result_dict.keys()
    w = csv.writer(f,delimiter=',')
    w.writerow(result_dict.keys())
    for v in zip(id,info):
      w.writerow(v)

# # 문장 테스트
# def test_sentences(sentences):

#     # 평가모드로 변경
#     model.eval()

#     eval_loss, eval_accuracy = 0, 0
#     nb_eval_steps, nb_eval_examples = 0, 0

#     # 문장을 입력 데이터로 변환
#     inputs, masks = convert_input_data(sentences)

#     # 데이터를 GPU에 넣음
#     b_input_ids = inputs.to(device)
#     b_input_mask = masks.to(device)
            
#     # 그래디언트 계산 안함
#     with torch.no_grad():     
#         # Forward 수행
#         outputs = model(b_input_ids, 
#                         token_type_ids=None, 
#                         attention_mask=b_input_mask)

#     # 로스 구함
#     logits = outputs[0]

#     # CPU로 데이터 이동
#     logits = logits.detach().cpu().numpy()

#     return logits



# submission=[]
# submission.append(test_sentences(t_sentences))

# t_id = test['id'].values
# zip(t_id, submission)

# submission.to_csv('/content/submission.csv',index=False)

# submission