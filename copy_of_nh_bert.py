# -*- coding: utf-8 -*-
"""Copy of NH_BERT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XZE3Kictx1dwupgalphD7iq6dBozQ16V
"""

from google.colab import drive
drive.mount("/content/drive")

import os
from pathlib import Path

current_path = Path(os.getcwd())
print(current_path)

base_path = current_path / "drive" / "My Drive" / "NH_dacon"
os.chdir(base_path)

#!unzip 리그1.zip -d data

!pip install transformers

# !pip install konlpy

# !sudo apt-get install curl git
# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

import pandas as pd
import numpy as np
from collections import Counter

import re 
import time
import nltk
import random
import datetime
from tqdm import tqdm

import tensorflow as tf
import torch

# from transformers import ElectraModel, ElectraForSequenceClassification, ElectraTokenizer

from transformers import BertTokenizer
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import TensorDataset, DataLoader,RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# vocab tokenization
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import re
import unicodedata
import six
# from konlpy.tag import Mecab

!pwd

train = pd.read_csv('./news_train.csv')
# test = pd.read_csv('/content/news_test.csv')
# submission = pd.read_csv('/content/sample_submission.csv')

train.head(1)

# test.head(1)

# submission.head(1)

print(train.shape)
# print(test.shape)

sentences = train['content']
sentences[:10]

# regex_tokenizer = nltk.RegexpTokenizer("\w+")

# def normalize_text(text):
#   # lowercase text
#   text = str(text).lower()
#   # remove non-UTF
#   text = text.encode("utf-8", "ignore").decode()
#   # remove punktuation symbols
#   text = " ".join(regex_tokenizer.tokenize(text))
#   return text

# sentences = [normalize_text(sentence) for sentence in sentences]
# sentences[:5]

labels = train['info'].values
labels

# t_id = test['id'].values.astype(str)
# t_id

# """Tokenization classes."""


# # # import tensorflow as tf


# # def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):
# #   """Checks whether the casing config is consistent with the checkpoint name."""

# #   # The casing has to be passed in by the user and there is no explicit check
# #   # as to whether it matches the checkpoint. The casing information probably
# #   # should have been stored in the bert_config.json file, but it's not, so
# #   # we have to heuristically detect it to validate.

# #   if not init_checkpoint:
# #     return

# #   m = re.match("^.*?([A-Za-z0-9_-]+)/bert_model.ckpt", init_checkpoint)
# #   if m is None:
# #     return

# #   model_name = m.group(1)

# #   lower_models = [
# #       "uncased_L-24_H-1024_A-16", "uncased_L-12_H-768_A-12",
# #       "multilingual_L-12_H-768_A-12", "chinese_L-12_H-768_A-12"
# #   ]

# #   cased_models = [
# #       "cased_L-12_H-768_A-12", "cased_L-24_H-1024_A-16",
# #       "multi_cased_L-12_H-768_A-12"
# #   ]

# #   is_bad_config = False
# #   if model_name in lower_models and not do_lower_case:
# #     is_bad_config = True
# #     actual_flag = "False"
# #     case_name = "lowercased"
# #     opposite_flag = "True"

# #   if model_name in cased_models and do_lower_case:
# #     is_bad_config = True
# #     actual_flag = "True"
# #     case_name = "cased"
# #     opposite_flag = "False"

# #   if is_bad_config:
# #     raise ValueError(
# #         "You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. "
# #         "However, `%s` seems to be a %s model, so you "
# #         "should pass in `--do_lower_case=%s` so that the fine-tuning matches "
# #         "how the model was pre-training. If this error is wrong, please "
# #         "just comment out this check." % (actual_flag, init_checkpoint,
# #                                           model_name, case_name, opposite_flag))


# def convert_to_unicode(text):
#   """Converts `text` to Unicode (if it's not already), assuming utf-8 input."""
#   if six.PY3:
#     if isinstance(text, str):
#       return text
#     elif isinstance(text, bytes):
#       return text.decode("utf-8", "ignore")
#     else:
#       raise ValueError("Unsupported string type: %s" % (type(text)))
#   elif six.PY2:
#     if isinstance(text, str):
#       return text.decode("utf-8", "ignore")
#     elif isinstance(text, unicode):
#       return text
#     else:
#       raise ValueError("Unsupported string type: %s" % (type(text)))
#   else:
#     raise ValueError("Not running on Python2 or Python 3?")


# def printable_text(text):
#   """Returns text encoded in a way suitable for print or `tf.logging`."""

#   # These functions want `str` for both Python2 and Python3, but in one case
#   # it's a Unicode string and in the other it's a byte string.
#   if six.PY3:
#     if isinstance(text, str):
#       return text
#     elif isinstance(text, bytes):
#       return text.decode("utf-8", "ignore")
#     else:
#       raise ValueError("Unsupported string type: %s" % (type(text)))
#   elif six.PY2:
#     if isinstance(text, str):
#       return text
#     elif isinstance(text, unicode):
#       return text.encode("utf-8")
#     else:
#       raise ValueError("Unsupported string type: %s" % (type(text)))
#   else:
#     raise ValueError("Not running on Python2 or Python 3?")


# def load_vocab(vocab_file):
#   """Loads a vocabulary file into a dictionary."""
#   vocab = collections.OrderedDict()
#   index = 0
#   # with tf.gfile.GFile(vocab_file, "r") as reader:
#   with open(vocab_file, "r") as reader:
#     while True:
#       token = convert_to_unicode(reader.readline())
#       if not token:
#         break
#       token = token.strip()
#       vocab[token] = index
#       index += 1
#   return vocab


# def convert_by_vocab(vocab, items):
#   """Converts a sequence of [tokens|ids] using the vocab."""
#   output = []
#   for item in items:
#     output.append(vocab[item])
#   return output


# def convert_tokens_to_ids(vocab, tokens):
#   return convert_by_vocab(vocab, tokens)


# def convert_ids_to_tokens(inv_vocab, ids):
#   return convert_by_vocab(inv_vocab, ids)


# def whitespace_tokenize(text):
#   """Runs basic whitespace cleaning and splitting on a piece of text."""
#   text = text.strip()
#   if not text:
#     return []
#   tokens = text.split()
#   return tokens


# class FullTokenizer(object):
#   """Runs end-to-end tokenziation."""

#   def __init__(self, vocab_file, do_lower_case=True):
#     self.vocab = load_vocab(vocab_file)
#     self.inv_vocab = {v: k for k, v in self.vocab.items()}
#     self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
#     self.mecab_tokenizer = MecabTokenizer(vocab=self.vocab)

#   def tokenize(self, text):
#     split_tokens = []
#     # print(text)
#     for token in self.basic_tokenizer.tokenize(text):
#       for sub_token in self.mecab_tokenizer.tokenize(token):
#         split_tokens.append(sub_token)
#     # print(split_tokens[:3])
#     return split_tokens

#   def convert_tokens_to_ids(self, tokens):
#     return convert_by_vocab(self.vocab, tokens)

#   def convert_ids_to_tokens(self, ids):
#     return convert_by_vocab(self.inv_vocab, ids)



# class BasicTokenizer(object):
#   """Runs basic tokenization (punctuation splitting, lower casing, etc.)."""

#   def __init__(self, do_lower_case=True):
#     """Constructs a BasicTokenizer.

#     Args:
#       do_lower_case: Whether to lower case the input.
#     """
#     self.do_lower_case = do_lower_case

#   def tokenize(self, text):
#     """Tokenizes a piece of text."""
#     text = convert_to_unicode(text)
#     text = self._clean_text(text)

#     # This was added on November 1st, 2018 for the multilingual and Chinese
#     # models. This is also applied to the English models now, but it doesn't
#     # matter since the English models were not trained on any Chinese data
#     # and generally don't have any Chinese data in them (there are Chinese
#     # characters in the vocabulary because Wikipedia does have some Chinese
#     # words in the English Wikipedia.).
#     # text = self._tokenize_chinese_chars(text)

#     orig_tokens = whitespace_tokenize(text)
#     split_tokens = []
#     for token in orig_tokens:
#       if self.do_lower_case:
#         token = token.lower()
#         token = self._run_strip_accents(token)
#       split_tokens.extend(self._run_split_on_punc(token))

#     output_tokens = whitespace_tokenize(" ".join(split_tokens))
#     return output_tokens

#   def _run_strip_accents(self, text):
#     """Strips accents from a piece of text."""
#     # text = unicodedata.normalize("NFD", text)
#     text = unicodedata.normalize("NFKC", text)
#     output = []
#     for char in text:
#       cat = unicodedata.category(char)
#       if cat == "Mn":
#         continue
#       output.append(char)
#     return "".join(output)

#   def _run_split_on_punc(self, text):
#     """Splits punctuation on a piece of text."""
#     chars = list(text)
#     i = 0
#     start_new_word = True
#     output = []
#     while i < len(chars):
#       char = chars[i]
#       if not _is_punctuation(char) or char =='#':
#         if start_new_word:
#           output.append([])
#         start_new_word = False
#         output[-1].append(char)
#       else:
#         output.append([char])
#         start_new_word = True
#       i += 1

#     return ["".join(x) for x in output]

#   def _tokenize_chinese_chars(self, text):
#     """Adds whitespace around any CJK character."""
#     output = []
#     for char in text:
#       cp = ord(char)
#       if self._is_chinese_char(cp):
#         output.append(" ")
#         output.append(char)
#         output.append(" ")
#       else:
#         output.append(char)
#     return "".join(output)

#   def _is_chinese_char(self, cp):
#     """Checks whether CP is the codepoint of a CJK character."""
#     # This defines a "chinese character" as anything in the CJK Unicode block:
#     #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
#     #
#     # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
#     # despite its name. The modern Korean Hangul alphabet is a different block,
#     # as is Japanese Hiragana and Katakana. Those alphabets are used to write
#     # space-separated words, so they are not treated specially and handled
#     # like the all of the other languages.
#     if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #
#         (cp >= 0x3400 and cp <= 0x4DBF) or  #
#         (cp >= 0x20000 and cp <= 0x2A6DF) or  #
#         (cp >= 0x2A700 and cp <= 0x2B73F) or  #
#         (cp >= 0x2B740 and cp <= 0x2B81F) or  #
#         (cp >= 0x2B820 and cp <= 0x2CEAF) or
#         (cp >= 0xF900 and cp <= 0xFAFF) or  #
#         (cp >= 0x2F800 and cp <= 0x2FA1F)):  #
#       return True

#     return False

#   def _clean_text(self, text):
#     """Performs invalid character removal and whitespace cleanup on text."""
#     output = []
#     for char in text:
#       cp = ord(char)
#       if cp == 0 or cp == 0xfffd or _is_control(char):
#         continue
#       if _is_whitespace(char):
#         output.append(" ")
#       else:
#         output.append(char)
#     return "".join(output)

# class MecabTokenizer(object):
#   """Runs Mecab tokenziation."""

#   def __init__(self, vocab, unk_token="[UNK]"):
#     self.vocab = vocab
#     self.unk_token = unk_token

#   def tokenize(self, text):
#     text = convert_to_unicode(text)
#     mecab = Mecab()

#     output_tokens = []
#     for w in whitespace_tokenize(text): # 단어 w
#       count = 0
#       for token in mecab.morphs(w): # 단어 내에서 tokenize
#           tk = token
#           if count > 0:
#             tk = "##" + tk
#           else:
#             count += 1
#           if tk not in self.vocab:
#             output_tokens.append(self.unk_token)
#           else:
#             output_tokens.append(tk)

#     return output_tokens

# def _is_whitespace(char):
#   """Checks whether `chars` is a whitespace character."""
#   # \t, \n, and \r are technically contorl characters but we treat them
#   # as whitespace since they are generally considered as such.
#   if char == " " or char == "\t" or char == "\n" or char == "\r":
#     return True
#   cat = unicodedata.category(char)
#   if cat == "Zs":
#     return True
#   return False


# def _is_control(char):
#   """Checks whether `chars` is a control character."""
#   # These are technically control characters but we count them as whitespace
#   # characters.
#   if char == "\t" or char == "\n" or char == "\r":
#     return False
#   cat = unicodedata.category(char)
#   if cat in ("Cc", "Cf"):
#     return True
#   return False


# def _is_punctuation(char):
#   """Checks whether `chars` is a punctuation character."""
#   cp = ord(char)
#   # We treat all non-letter/number ASCII as punctuation.
#   # Characters such as "^", "$", and "`" are not in the Unicode
#   # Punctuation class but we treat them as punctuation anyways, for
#   # consistency.
#   if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or
#       (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):
#     return True
#   cat = unicodedata.category(char)
#   if cat.startswith("P"):
#     return True
#   return False

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=False)
# tokenizer = FullTokenizer(vocab_file='/content/vocab.txt', do_lower_case=False)
# tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

print(sentences[0])
print(tokenizer.tokenize('코스닥 기관 현재 678억 순매도'))
print(tokenized_texts[0])

# sentences = ["[CLS]"+str(sentence)+"[SEP]" for sentence in sentences]
# sentences[:5]
for tokens in tokenized_texts:
  tokens.insert(0, "[CLS]")
  tokens.append("[SEP]")
# sentences = ["[CLS]"+str(sentence)+ for sentence in sentences]
tokenized_texts[:5]

input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
print('Max sentence length: ', sum([len(sen) for sen in input_ids])/len(input_ids))
MAX_LEN = 128
input_ids = pad_sequences(input_ids,maxlen=MAX_LEN, dtype="long",truncating = "post",padding = "post")
input_ids[0]

attention_masks=[]
for seq in input_ids:
  seq_mask = [float(i>0) for i in seq]
  attention_masks.append(seq_mask)

print(attention_masks[0])

train_inputs, validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=2020,test_size=0.1)
train_masks, validation_masks,_,_=train_test_split(attention_masks,input_ids,random_state=2020,test_size=0.1)

train_inputs = torch.tensor(train_inputs)
train_labels = torch.tensor(train_labels)
train_masks = torch.tensor(train_masks)
validation_inputs = torch.tensor(validation_inputs)
validation_labels = torch.tensor(validation_labels)
validation_masks = torch.tensor(validation_masks)

print(train_inputs[0])
print(train_labels[0])
print(train_masks[0])
print(validation_inputs[0])
print(validation_labels[0])
print(validation_masks[0])

batch_size=32

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

"""#test"""

# t_sentences = test['content']
# t_sentences[:10]

# t_sentences = [normalize_text(sentence) for sentence in t_sentences]
# t_sentences[:5]

device_name = tf.test.gpu_device_name()

if device_name =='/device:GPU:0':
  print('found GPU at: {}'.format(device_name))
else:
  raise SystemError('GPU device not found')

if torch.cuda.is_available():
  device = torch.device("cuda")
  print('the are %d GPU(s) abailable.'%torch.cuda.device_count())
  print('We will use the GPU:',torch.cuda.get_device_name(0))
else:
  device = torch.device("cpu")
  print('No GPU available, using the CPU instead.')

model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased",num_labels=2)
# model = ElectraForSequenceClassification.from_pretrained("monologg/koelectra-small-v3-discriminator",num_labels=2)
model.cuda()

optimizer = AdamW(model.parameters(),
                  lr = 2e-5,
                  eps = 1e-8)
epochs = 3
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)

def flat_accuracy(preds, labels):
    
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    return np.sum(pred_flat == labels_flat) / len(labels_flat)

# 시간 표시 함수
def format_time(elapsed):

    # 반올림
    elapsed_rounded = int(round((elapsed)))
    
    # hh:mm:ss으로 형태 변경
    return str(datetime.timedelta(seconds=elapsed_rounded))

# 재현을 위해 랜덤시드 고정
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# 그래디언트 초기화
model.zero_grad()

# 에폭만큼 반복
for epoch_i in range(0, epochs):
    
    # ========================================
    #               Training
    # ========================================
    
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    t0 = time.time()
    total_loss = 0
    model.train()
        
    for step, batch in enumerate(train_dataloader):
        if step % 500 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        batch = tuple(t.to(device) for t in batch)

        b_input_ids, b_input_mask, b_labels = batch

        outputs = model(b_input_ids, 
                        token_type_ids=None, 
                        attention_mask=b_input_mask, 
                        labels=b_labels)

        loss = outputs[0]
        total_loss += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()
        model.zero_grad()

    avg_train_loss = total_loss / len(train_dataloader)            

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epoch took: {:}".format(format_time(time.time() - t0)))
        
    # ========================================
    #               Validation
    # ========================================

    print("")
    print("Running Validation...")

    t0 = time.time()

    model.eval()

    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0

    for batch in validation_dataloader:
        batch = tuple(t.to(device) for t in batch)

        b_input_ids, b_input_mask, b_labels = batch

        with torch.no_grad():     
            outputs = model(b_input_ids, 
                            token_type_ids=None, 
                            attention_mask=b_input_mask)

        logits = outputs[0]
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        tmp_eval_accuracy = flat_accuracy(logits, label_ids)
        eval_accuracy += tmp_eval_accuracy
        nb_eval_steps += 1

    print("  Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
    print("  Validation took: {:}".format(format_time(time.time() - t0)))

    output_file = "./NH_bert-multi-128_{}.pt".format(epoch_i + 1)
    torch.save(model.state_dict(), output_file)
print("")
print("Training complete!")

"""Test Inference"""

test = pd.read_csv('./news_test.csv')

import time
start_t = time.time()

import pandas as pd
import numpy as np

import datetime

import torch

from transformers import BertTokenizer
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import TensorDataset, DataLoader,RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences

# load tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=False)
# load model
model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',num_labels=2)
model.load_state_dict(torch.load("./NH_bert-multi-128_{}.pt"))
model.cuda()

# tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
# tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

def convert_input_data(sentences):

    # BERT의 토크나이저로 문장을 토큰으로 분리
    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

    # 입력 토큰의 최대 시퀀스 길이
    MAX_LEN = 128

    # 토큰을 숫자 인덱스로 변환
    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
    
    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움
    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

    # 어텐션 마스크 초기화
    attention_masks = []

    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정
    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상
    for seq in input_ids:
        seq_mask = [float(i>0) for i in seq]
        attention_masks.append(seq_mask)

    # 데이터를 파이토치의 텐서로 변환
    inputs = torch.tensor(input_ids)
    masks = torch.tensor(attention_masks)

    return inputs, masks

t_sentences = test['content']
test_inputs, test_masks = convert_input_data(t_sentences)
test_data = TensorDataset(test_inputs, test_masks)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

# 시간 표시 함수
def format_time(elapsed):

    # 반올림
    elapsed_rounded = int(round((elapsed)))
    
    # hh:mm:ss으로 형태 변경
    return str(datetime.timedelta(seconds=elapsed_rounded))

model.eval()
eval_loss, eval_accuracy = 0, 0
nb_eval_steps, nb_eval_examples = 0, 0

result_dict = {"info":[],"id":[]}
result_dict["id"].extend(t_id)

# 데이터로더에서 배치만큼 반복하여 가져옴
for step, batch in enumerate(test_dataloader):
    # 경과 정보 표시
    if step % 100 == 0 and not step == 0:
        elapsed = format_time(time.time() - t0)
        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))

    # 배치를 GPU에 넣음
    batch = tuple(t.to(device) for t in batch)
    
    # 배치에서 데이터 추출
    b_input_ids, b_input_mask = batch
    
    # 그래디언트 계산 안함
    with torch.no_grad():     
        # Forward 수행
        outputs = model(b_input_ids, 
                        token_type_ids=None, 
                        attention_mask=b_input_mask)
    
    # 로스 구함
    logits = outputs[0]
#   logits = outputs[0].argmax(1).item()
    # CPU로 데이터 이동
    logits = logits.detach().cpu().numpy()
    # id = b_id.to('cpu').numpy()
    predicted_label = np.argmax(logits, axis=1).flatten()
    result_dict["info"].extend(predicted_label)
    
    
#     # 출력 로짓과 라벨을 비교하여 정확도 계산
#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)
#     eval_accuracy += tmp_eval_accuracy
#     nb_eval_steps += 1

# print("")
# print("Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
print("Test took: {:}".format(format_time(time.time() - t0)))

print('Test Inference time', time.time() - start_t)

import csv
info = result_dict["info"]
id = result_dict["id"]
with open('/content/submission_bert-multi-128_epoch3.csv','w') as f:
    # fieldnames = result_dict.keys()
    w = csv.writer(f,delimiter=',')
    w.writerow(result_dict.keys())
    for v in zip(id,info):
      w.writerow(v)

# # 문장 테스트
# def test_sentences(sentences):

#     # 평가모드로 변경
#     model.eval()

#     eval_loss, eval_accuracy = 0, 0
#     nb_eval_steps, nb_eval_examples = 0, 0

#     # 문장을 입력 데이터로 변환
#     inputs, masks = convert_input_data(sentences)

#     # 데이터를 GPU에 넣음
#     b_input_ids = inputs.to(device)
#     b_input_mask = masks.to(device)
            
#     # 그래디언트 계산 안함
#     with torch.no_grad():     
#         # Forward 수행
#         outputs = model(b_input_ids, 
#                         token_type_ids=None, 
#                         attention_mask=b_input_mask)

#     # 로스 구함
#     logits = outputs[0]

#     # CPU로 데이터 이동
#     logits = logits.detach().cpu().numpy()

#     return logits

# submission=[]
# submission.append(test_sentences(t_sentences))

# t_id = test['id'].values
# zip(t_id, submission)

# submission.to_csv('/content/submission.csv',index=False)

# submission

